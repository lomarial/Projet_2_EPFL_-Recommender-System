{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "path_dataset = \"data_train.csv\"\n",
    "path_sample = \"sampleSubmission.csv\"\n",
    "path_submission = \"submission.csv\"\n",
    "ratings_full = load_data(path_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract only a subset of the ratings to work with, to quickly tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "row, col = ratings_full.shape\n",
    "select = np.random.choice(row, size=3000)\n",
    "ratings = ratings_full[:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.5):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][: , valid_users]  \n",
    "    \n",
    "    # init\n",
    "    num_rows, num_cols = valid_ratings.shape\n",
    "    train = sp.lil_matrix((num_rows, num_cols))\n",
    "    test = sp.lil_matrix((num_rows, num_cols))\n",
    "    \n",
    "    print(\"the shape of original ratings. (# of row, # of col): {}\".format(\n",
    "        ratings.shape))\n",
    "    print(\"the shape of valid ratings. (# of row, # of col): {}\".format(\n",
    "        (num_rows, num_cols)))\n",
    "    \n",
    "    def list_transpose(L):\n",
    "        \"\"\"Transforms a row vector into a column.\"\"\"\n",
    "        R = []\n",
    "        for e in L:\n",
    "            R.append([e])\n",
    "        return R\n",
    "\n",
    "    nz_items, nz_users = valid_ratings.nonzero()\n",
    "    \n",
    "    # split the data\n",
    "    for user in set(nz_users):\n",
    "        # randomly select a subset of ratings\n",
    "        row, col = valid_ratings[:, user].nonzero()\n",
    "        selects = np.random.choice(row, size=int(len(row) * p_test))\n",
    "        residual = list(set(row) - set(selects))\n",
    "\n",
    "        # add to train set\n",
    "        train[list_transpose(residual), user] = valid_ratings[residual, user]\n",
    "\n",
    "        # add to test set\n",
    "        test[list_transpose(selects), user] = valid_ratings[selects, user]\n",
    "        \n",
    "    print(\"Total number of nonzero elements in original data:{v}\".format(v=ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    return valid_ratings, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of original ratings. (# of row, # of col): (3000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (2966, 982)\n",
      "Total number of nonzero elements in original data:353931\n",
      "Total number of nonzero elements in train data:214380\n",
      "Total number of nonzero elements in test data:138842\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plots import plot_train_test_data\n",
    "\n",
    "valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=20, p_test=0.5)\n",
    "plot_train_test_data(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the global mean to do the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE of baseline using the global mean: [[1.11439266]].\n"
     ]
    }
   ],
   "source": [
    "from helpers import calculate_mse\n",
    "\n",
    "def baseline_global_mean(train, test):\n",
    "    \"\"\"baseline method: use the global mean.\"\"\"\n",
    "    # find the non zero ratings in the train\n",
    "    nonzero_train = train[train.nonzero()]\n",
    "\n",
    "    # calculate the global mean\n",
    "    global_mean_train = nonzero_train.mean()\n",
    "\n",
    "    # find the non zero ratings in the test\n",
    "    nonzero_test = test[test.nonzero()].todense()\n",
    "\n",
    "    # predict the ratings as global mean\n",
    "    mse = calculate_mse(nonzero_test, global_mean_train)\n",
    "    rmse = np.sqrt(1.0 * mse / nonzero_test.shape[1])\n",
    "    print(\"test RMSE of baseline using the global mean: {v}.\".format(v=rmse))\n",
    "\n",
    "baseline_global_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the user means as the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE of the baseline using the user mean: [[1.02631872]].\n"
     ]
    }
   ],
   "source": [
    "def baseline_user_mean(train, test):\n",
    "    \"\"\"baseline method: use the user means as the prediction.\"\"\"\n",
    "    mse = 0\n",
    "    num_items, num_users = train.shape\n",
    "    \n",
    "    for user_index in range(num_users):\n",
    "        # find the non-zero ratings for each user in the training dataset\n",
    "        train_ratings = train[:, user_index]\n",
    "        nonzeros_train_ratings = train_ratings[train_ratings.nonzero()]\n",
    "        \n",
    "        # calculate the mean if the number of elements is not 0\n",
    "        if nonzeros_train_ratings.shape[0] != 0:\n",
    "            user_train_mean = nonzeros_train_ratings.mean()\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # find the non-zero ratings for each user in the test dataset\n",
    "        test_ratings = test[:, user_index]\n",
    "        nonzeros_test_ratings = test_ratings[test_ratings.nonzero()].todense()\n",
    "        \n",
    "        # calculate the test error \n",
    "        mse += calculate_mse(nonzeros_test_ratings, user_train_mean)\n",
    "    rmse = np.sqrt(1.0 * mse / test.nnz)\n",
    "    print(\"test RMSE of the baseline using the user mean: {v}.\".format(v=rmse))\n",
    "\n",
    "baseline_user_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the item means as the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE of the baseline using the item mean: [[1.09351351]].\n"
     ]
    }
   ],
   "source": [
    "def baseline_item_mean(train, test):\n",
    "    \"\"\"baseline method: use item means as the prediction.\"\"\"\n",
    "    mse = 0\n",
    "    num_items, num_users = train.shape\n",
    "    \n",
    "    for item_index in range(num_items):\n",
    "        # find the non-zero ratings for each item in the training dataset\n",
    "        train_ratings = train[item_index, :]\n",
    "        nonzeros_train_ratings = train_ratings[train_ratings.nonzero()]\n",
    "\n",
    "        # calculate the mean if the number of elements is not 0\n",
    "        if nonzeros_train_ratings.shape[0] != 0:\n",
    "            item_train_mean = nonzeros_train_ratings.mean()\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # find the non-zero ratings for each movie in the test dataset\n",
    "        test_ratings = test[item_index, :]\n",
    "        nonzeros_test_ratings = test_ratings[test_ratings.nonzero()].todense()\n",
    "        \n",
    "        # calculate the test error \n",
    "        mse += calculate_mse(nonzeros_test_ratings, item_train_mean)\n",
    "    rmse = np.sqrt(1.0 * mse / test.nnz)\n",
    "    print(\"test RMSE of the baseline using the item mean: {v}.\".format(v=rmse))\n",
    "    \n",
    "baseline_item_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the Matrix Factorization using ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_MF(train, num_features):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "    \n",
    "    num_item, num_user = train.get_shape()\n",
    "\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # Initialize randomly\n",
    "    user_features = np.random.rand(num_features, num_user)\n",
    "    item_features = np.random.rand(num_features, num_item)\n",
    "    \n",
    "    # Set the first row of item features to the mean of the ratings of that item.\n",
    "    item_nnz = train.getnnz(axis=1)\n",
    "    item_sum = train.sum(axis=1)\n",
    "\n",
    "    for ind in range(num_item):\n",
    "        item_features[0, ind] = item_sum[ind, 0] / item_nnz[ind]\n",
    "    \n",
    "    # Do the same for user features.\n",
    "    user_nnz = train.getnnz(axis=0)\n",
    "    user_sum = train.sum(axis=0)\n",
    "\n",
    "    for ind in range(num_user):\n",
    "        user_features[0, ind] = user_sum[0, ind] / user_nnz[ind]\n",
    "    \n",
    "    return user_features, item_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cost by the method of matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(data, user_features, item_features):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "    nz_row, nz_col = data.nonzero()\n",
    "    nz = list(zip(nz_row, nz_col))\n",
    "    mse = 0\n",
    "    for row, col in nz:\n",
    "        item_info = item_features[:, row]\n",
    "        user_info = user_features[:, col]\n",
    "        predicted_rating = int(round(user_info.T.dot(item_info))) # To calculate the RMSE as would do the aicrowd platform\n",
    "        if predicted_rating not in [1, 2, 3, 4, 5]:\n",
    "            if predicted_rating > 5:\n",
    "                predicted_rating = 5\n",
    "            elif predicted_rating < 1:\n",
    "                predicted_rating = 1\n",
    "        mse += (data[row, col] - predicted_rating) ** 2\n",
    "    return np.sqrt(1.0 * mse / len(nz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_feature(\n",
    "        train, item_features, lambda_user,\n",
    "        nnz_items_per_user, nz_user_itemindices):\n",
    "    \"\"\"update user feature matrix.\"\"\"\n",
    "    \"\"\"the best lambda is assumed to be nnz_items_per_user[user] * lambda_user\"\"\"\n",
    "    num_user = nnz_items_per_user.shape[0]\n",
    "    num_feature = item_features.shape[0]\n",
    "    lambda_I = lambda_user * sp.eye(num_feature)\n",
    "    updated_user_features = np.zeros((num_feature, num_user))\n",
    "\n",
    "    for user, items in nz_user_itemindices:\n",
    "        # extract the columns corresponding to the prediction for given item\n",
    "        M = item_features[:, items]\n",
    "        \n",
    "        # update column row of user features\n",
    "        V = M @ train[items, user]\n",
    "        A = M @ M.T + nnz_items_per_user[user] * lambda_I\n",
    "        X = np.linalg.solve(A, V)\n",
    "        updated_user_features[:, user] = np.copy(X.T)\n",
    "    return updated_user_features\n",
    "\n",
    "def update_item_feature(\n",
    "        train, user_features, lambda_item,\n",
    "        nnz_users_per_item, nz_item_userindices):\n",
    "    \"\"\"update item feature matrix.\"\"\"\n",
    "    \"\"\"the best lambda is assumed to be nnz_items_per_item[item] * lambda_item\"\"\"\n",
    "    num_item = nnz_users_per_item.shape[0]\n",
    "    num_feature = user_features.shape[0]\n",
    "    lambda_I = lambda_item * sp.eye(num_feature)\n",
    "    updated_item_features = np.zeros((num_feature, num_item))\n",
    "\n",
    "    for item, users in nz_item_userindices:\n",
    "        # extract the columns corresponding to the prediction for given user\n",
    "        M = user_features[:, users]\n",
    "        V = M @ train[item, users].T\n",
    "        A = M @ M.T + nnz_users_per_item[item] * lambda_I\n",
    "        X = np.linalg.solve(A, V)\n",
    "        updated_item_features[:, item] = np.copy(X.T)\n",
    "    return updated_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import build_index_groups\n",
    "\n",
    "\n",
    "def ALS(train, params):\n",
    "    \"\"\"Alternating Least Squares (ALS) algorithm.\"\"\"\n",
    "    # define parameters\n",
    "    num_features = params['K']   # K in the lecture notes\n",
    "    lambda_user = params['lambda_user']\n",
    "    lambda_item = params['lambda_item']\n",
    "    stop_criterion = params['stop_criterion']\n",
    "    change = 1\n",
    "    error_list = [0, 0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init ALS\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # get the number of non-zero ratings for each user and item\n",
    "    nnz_items_per_user, nnz_users_per_item = train.getnnz(axis=0), train.getnnz(axis=1)\n",
    "    \n",
    "    # group the indices by row or column index\n",
    "    nz_train, nz_item_userindices, nz_user_itemindices = build_index_groups(train)\n",
    "\n",
    "    # run ALS\n",
    "    print(\"\\nstart the ALS algorithm...\")\n",
    "    while change > stop_criterion:\n",
    "        # update user feature & item feature\n",
    "        user_features = update_user_feature(\n",
    "            train, item_features, lambda_user,\n",
    "            nnz_items_per_user, nz_user_itemindices)\n",
    "        item_features = update_item_feature(\n",
    "            train, user_features, lambda_item,\n",
    "            nnz_users_per_item, nz_item_userindices)\n",
    "\n",
    "        error = compute_error(train, user_features, item_features)\n",
    "        print(\"RMSE on training set: {}.\".format(error))\n",
    "        error_list.append(error)\n",
    "        change = np.fabs(error_list[-1] - error_list[-2])\n",
    "    \n",
    "    return item_features, user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define initial parameters\n",
    "params = {\n",
    "    \"K\": 50,\n",
    "    \"stop_criterion\": 1e-5,\n",
    "    \"lambda_user\": 0.1,\n",
    "    \"lambda_item\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start the ALS algorithm...\n",
      "RMSE on training set: 1.105573241099733.\n",
      "RMSE on training set: 0.9634494849097929.\n",
      "RMSE on training set: 0.8888005483670101.\n",
      "RMSE on training set: 0.8539405593848964.\n",
      "RMSE on training set: 0.8351477588330521.\n",
      "RMSE on training set: 0.8237965352024201.\n",
      "RMSE on training set: 0.8153388853114535.\n",
      "RMSE on training set: 0.8096782562393149.\n",
      "RMSE on training set: 0.8052415980062276.\n",
      "RMSE on training set: 0.8019358051267947.\n",
      "RMSE on training set: 0.7991885286460465.\n",
      "RMSE on training set: 0.7971869564710006.\n",
      "RMSE on training set: 0.7953768364551486.\n",
      "RMSE on training set: 0.7940855632143069.\n",
      "RMSE on training set: 0.7928304304366974.\n",
      "RMSE on training set: 0.7917559644424235.\n",
      "RMSE on training set: 0.7907537786123915.\n",
      "RMSE on training set: 0.7901990830881365.\n",
      "RMSE on training set: 0.7893810820952374.\n",
      "RMSE on training set: 0.7889170731672199.\n",
      "RMSE on training set: 0.7884113769475348.\n",
      "RMSE on training set: 0.7882338627882741.\n",
      "RMSE on training set: 0.7878550322387134.\n",
      "RMSE on training set: 0.7875204444426511.\n",
      "RMSE on training set: 0.7872657071096264.\n",
      "RMSE on training set: 0.787123492285342.\n",
      "RMSE on training set: 0.7870138508188568.\n",
      "RMSE on training set: 0.7867411625629974.\n",
      "RMSE on training set: 0.7865958878441922.\n",
      "RMSE on training set: 0.786601817949759.\n"
     ]
    }
   ],
   "source": [
    "item_features, user_features = ALS(train, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE after running ALS: 1.0405521303487224.\n"
     ]
    }
   ],
   "source": [
    "rmse = compute_error(test, user_features, item_features)\n",
    "print(\"test RMSE after running ALS: {v}.\".format(v=rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning lambda_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['lambda_user'] = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start the ALS algorithm...\n",
      "RMSE on training set: 1.2946404334943493.\n",
      "RMSE on training set: 1.129164142866568.\n",
      "RMSE on training set: 1.0647061643217852.\n",
      "RMSE on training set: 1.0271684868069142.\n",
      "RMSE on training set: 1.0024319713889838.\n",
      "RMSE on training set: 0.9867013859142063.\n",
      "RMSE on training set: 0.9753013891620705.\n",
      "RMSE on training set: 0.9681705587369988.\n",
      "RMSE on training set: 0.9626381792780454.\n",
      "RMSE on training set: 0.9587416536034742.\n",
      "RMSE on training set: 0.9557643013617113.\n",
      "RMSE on training set: 0.9534310112229807.\n",
      "RMSE on training set: 0.9514695671214697.\n",
      "RMSE on training set: 0.9499780269577734.\n",
      "RMSE on training set: 0.948828336549846.\n",
      "RMSE on training set: 0.9477781501812562.\n",
      "RMSE on training set: 0.9469632699575811.\n",
      "RMSE on training set: 0.9462807915757727.\n",
      "RMSE on training set: 0.9457507306074072.\n",
      "RMSE on training set: 0.9452080349385715.\n",
      "RMSE on training set: 0.9448600524443306.\n",
      "RMSE on training set: 0.9446329309289159.\n",
      "RMSE on training set: 0.9442056957742081.\n",
      "RMSE on training set: 0.9439981823799232.\n",
      "RMSE on training set: 0.9438425173935031.\n",
      "RMSE on training set: 0.9436769407407739.\n",
      "RMSE on training set: 0.9435286384798726.\n",
      "RMSE on training set: 0.9433926742606601.\n",
      "RMSE on training set: 0.9432097096590907.\n",
      "RMSE on training set: 0.943120687018727.\n",
      "RMSE on training set: 0.9430415487278971.\n",
      "RMSE on training set: 0.942920355347962.\n",
      "RMSE on training set: 0.9428337791102167.\n",
      "RMSE on training set: 0.9427323511196379.\n",
      "RMSE on training set: 0.9427076109299194.\n",
      "RMSE on training set: 0.9426556544180021.\n",
      "RMSE on training set: 0.9425715282789918.\n",
      "RMSE on training set: 0.9424774960608931.\n",
      "RMSE on training set: 0.9424675973869157.\n"
     ]
    }
   ],
   "source": [
    "item_features, user_features = ALS(train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE after running ALS: 1.0395583861049025.\n"
     ]
    }
   ],
   "source": [
    "rmse = compute_error(test, user_features, item_features)\n",
    "print(\"test RMSE after running ALS: {v}.\".format(v=rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_user_list = [0.095,0.1,0.15,0.2]\n",
    "lambda_user_error = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start the ALS algorithm...\n",
      "RMSE on training set: 1.085551239943284.\n",
      "RMSE on training set: 0.9401434805233293.\n",
      "RMSE on training set: 0.8691035515407132.\n",
      "RMSE on training set: 0.8372368691994729.\n",
      "RMSE on training set: 0.8188725727032198.\n",
      "RMSE on training set: 0.8072058623586306.\n",
      "RMSE on training set: 0.7991622631176382.\n",
      "RMSE on training set: 0.7929186779572865.\n",
      "RMSE on training set: 0.7887219305909594.\n",
      "RMSE on training set: 0.7855158631061135.\n",
      "RMSE on training set: 0.782690096824254.\n",
      "RMSE on training set: 0.7806611620837853.\n",
      "RMSE on training set: 0.7790461849584713.\n",
      "RMSE on training set: 0.7772748363896999.\n",
      "RMSE on training set: 0.7763230568337482.\n",
      "RMSE on training set: 0.7753279959213687.\n",
      "RMSE on training set: 0.7744159884363285.\n",
      "RMSE on training set: 0.7734787832373262.\n",
      "RMSE on training set: 0.7727366517877482.\n",
      "RMSE on training set: 0.7722475416016824.\n",
      "RMSE on training set: 0.7714981796344139.\n",
      "RMSE on training set: 0.7712109327718042.\n",
      "RMSE on training set: 0.7708781974221037.\n",
      "RMSE on training set: 0.7706088791298165.\n",
      "RMSE on training set: 0.7701820135437982.\n",
      "RMSE on training set: 0.7699427442749905.\n",
      "RMSE on training set: 0.7696973403283623.\n",
      "RMSE on training set: 0.7694457958342052.\n",
      "RMSE on training set: 0.7694397335064395.\n",
      "test RMSE after running ALS: 1.0425575026189107.\n",
      "\n",
      "\n",
      "start the ALS algorithm...\n",
      "RMSE on training set: 1.105573241099733.\n",
      "RMSE on training set: 0.9634494849097929.\n",
      "RMSE on training set: 0.8888005483670101.\n",
      "RMSE on training set: 0.8539405593848964.\n",
      "RMSE on training set: 0.8351477588330521.\n",
      "RMSE on training set: 0.8237965352024201.\n",
      "RMSE on training set: 0.8153388853114535.\n",
      "RMSE on training set: 0.8096782562393149.\n",
      "RMSE on training set: 0.8052415980062276.\n",
      "RMSE on training set: 0.8019358051267947.\n",
      "RMSE on training set: 0.7991885286460465.\n",
      "RMSE on training set: 0.7971869564710006.\n",
      "RMSE on training set: 0.7953768364551486.\n",
      "RMSE on training set: 0.7940855632143069.\n",
      "RMSE on training set: 0.7928304304366974.\n",
      "RMSE on training set: 0.7917559644424235.\n",
      "RMSE on training set: 0.7907537786123915.\n",
      "RMSE on training set: 0.7901990830881365.\n",
      "RMSE on training set: 0.7893810820952374.\n",
      "RMSE on training set: 0.7889170731672199.\n",
      "RMSE on training set: 0.7884113769475348.\n",
      "RMSE on training set: 0.7882338627882741.\n",
      "RMSE on training set: 0.7878550322387134.\n",
      "RMSE on training set: 0.7875204444426511.\n",
      "RMSE on training set: 0.7872657071096264.\n",
      "RMSE on training set: 0.787123492285342.\n",
      "RMSE on training set: 0.7870138508188568.\n",
      "RMSE on training set: 0.7867411625629974.\n",
      "RMSE on training set: 0.7865958878441922.\n",
      "RMSE on training set: 0.786601817949759.\n",
      "test RMSE after running ALS: 1.0405521303487224.\n",
      "\n",
      "\n",
      "start the ALS algorithm...\n",
      "RMSE on training set: 1.2946404334943493.\n",
      "RMSE on training set: 1.129164142866568.\n",
      "RMSE on training set: 1.0647061643217852.\n",
      "RMSE on training set: 1.0271684868069142.\n",
      "RMSE on training set: 1.0024319713889838.\n",
      "RMSE on training set: 0.9867013859142063.\n",
      "RMSE on training set: 0.9753013891620705.\n",
      "RMSE on training set: 0.9681705587369988.\n",
      "RMSE on training set: 0.9626381792780454.\n",
      "RMSE on training set: 0.9587416536034742.\n",
      "RMSE on training set: 0.9557643013617113.\n",
      "RMSE on training set: 0.9534310112229807.\n",
      "RMSE on training set: 0.9514695671214697.\n",
      "RMSE on training set: 0.9499780269577734.\n",
      "RMSE on training set: 0.948828336549846.\n",
      "RMSE on training set: 0.9477781501812562.\n",
      "RMSE on training set: 0.9469632699575811.\n",
      "RMSE on training set: 0.9462807915757727.\n",
      "RMSE on training set: 0.9457507306074072.\n",
      "RMSE on training set: 0.9452080349385715.\n",
      "RMSE on training set: 0.9448600524443306.\n",
      "RMSE on training set: 0.9446329309289159.\n",
      "RMSE on training set: 0.9442056957742081.\n",
      "RMSE on training set: 0.9439981823799232.\n",
      "RMSE on training set: 0.9438425173935031.\n",
      "RMSE on training set: 0.9436769407407739.\n",
      "RMSE on training set: 0.9435286384798726.\n",
      "RMSE on training set: 0.9433926742606601.\n",
      "RMSE on training set: 0.9432097096590907.\n",
      "RMSE on training set: 0.943120687018727.\n",
      "RMSE on training set: 0.9430415487278971.\n",
      "RMSE on training set: 0.942920355347962.\n",
      "RMSE on training set: 0.9428337791102167.\n",
      "RMSE on training set: 0.9427323511196379.\n",
      "RMSE on training set: 0.9427076109299194.\n",
      "RMSE on training set: 0.9426556544180021.\n",
      "RMSE on training set: 0.9425715282789918.\n",
      "RMSE on training set: 0.9424774960608931.\n",
      "RMSE on training set: 0.9424675973869157.\n",
      "test RMSE after running ALS: 1.0395583861049025.\n",
      "\n",
      "\n",
      "start the ALS algorithm...\n",
      "RMSE on training set: 1.424483709621671.\n",
      "RMSE on training set: 1.2016990490692772.\n",
      "RMSE on training set: 1.1251934352883217.\n",
      "RMSE on training set: 1.0931859443431449.\n",
      "RMSE on training set: 1.0752670702296148.\n",
      "RMSE on training set: 1.063634439073504.\n",
      "RMSE on training set: 1.05656336763462.\n",
      "RMSE on training set: 1.051542742890184.\n",
      "RMSE on training set: 1.0477631561134848.\n",
      "RMSE on training set: 1.0447805403339898.\n",
      "RMSE on training set: 1.0428477770729991.\n",
      "RMSE on training set: 1.0416528126702567.\n",
      "RMSE on training set: 1.0403555981014279.\n",
      "RMSE on training set: 1.0395302749136186.\n",
      "RMSE on training set: 1.0389467708985474.\n",
      "RMSE on training set: 1.0384707480436806.\n",
      "RMSE on training set: 1.0380214697659778.\n",
      "RMSE on training set: 1.0377046116731814.\n",
      "RMSE on training set: 1.0373269522653272.\n",
      "RMSE on training set: 1.0370706050569183.\n",
      "RMSE on training set: 1.036996387459147.\n",
      "RMSE on training set: 1.0368546845659847.\n",
      "RMSE on training set: 1.0367759523677975.\n",
      "RMSE on training set: 1.036676966263721.\n",
      "RMSE on training set: 1.0365352197030542.\n",
      "RMSE on training set: 1.0365307194947895.\n",
      "test RMSE after running ALS: 1.056335130961593.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lambda_user in lambda_user_list:\n",
    "    params['lambda_user'] = lambda_user\n",
    "    item_features, user_features = ALS(train, params)\n",
    "    rmse = compute_error(test, user_features, item_features)\n",
    "    print(\"test RMSE after running ALS: {v}.\\n\".format(v=rmse))\n",
    "    lambda_user_error.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda_user = lambda_user_list[int(lambda_user_error.index(min(lambda_user_error)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15\n"
     ]
    }
   ],
   "source": [
    "print(best_lambda_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['lambda_user'] = best_lambda_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning lambda item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['lambda_item'] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start the ALS algorithm...\n",
      "RMSE on training set: 0.652419124249112.\n",
      "RMSE on training set: 0.5316944101616854.\n",
      "RMSE on training set: 0.4692921228376998.\n",
      "RMSE on training set: 0.43123022986384896.\n",
      "RMSE on training set: 0.4052436412910037.\n",
      "RMSE on training set: 0.38650864871957424.\n",
      "RMSE on training set: 0.372765601267336.\n",
      "RMSE on training set: 0.3621725449666809.\n",
      "RMSE on training set: 0.35382705025310135.\n",
      "RMSE on training set: 0.34706531579318844.\n",
      "RMSE on training set: 0.34149673574472517.\n",
      "RMSE on training set: 0.33627307105814025.\n",
      "RMSE on training set: 0.3314669270443554.\n",
      "RMSE on training set: 0.32713948772012524.\n",
      "RMSE on training set: 0.3231801005125342.\n",
      "RMSE on training set: 0.31952216166041564.\n",
      "RMSE on training set: 0.31628676373489173.\n",
      "RMSE on training set: 0.313434906213198.\n",
      "RMSE on training set: 0.31134442093392883.\n",
      "RMSE on training set: 0.3092699709274024.\n",
      "RMSE on training set: 0.3073560923678372.\n",
      "RMSE on training set: 0.30520105125429126.\n",
      "RMSE on training set: 0.30346907399156425.\n",
      "RMSE on training set: 0.3021905903665014.\n",
      "RMSE on training set: 0.3005188783271222.\n",
      "RMSE on training set: 0.2989392572764607.\n",
      "RMSE on training set: 0.29771966633080044.\n",
      "RMSE on training set: 0.29625897703217746.\n",
      "RMSE on training set: 0.2949334268617862.\n",
      "RMSE on training set: 0.29368926069116214.\n",
      "RMSE on training set: 0.2924796753304205.\n",
      "RMSE on training set: 0.2913451308325214.\n",
      "RMSE on training set: 0.2906237601754935.\n",
      "RMSE on training set: 0.28969134389401435.\n",
      "RMSE on training set: 0.28878014702954663.\n",
      "RMSE on training set: 0.2879875711263233.\n",
      "RMSE on training set: 0.2872090495728581.\n",
      "RMSE on training set: 0.2865261079913777.\n",
      "RMSE on training set: 0.28579257387029916.\n",
      "RMSE on training set: 0.28459040213376696.\n",
      "RMSE on training set: 0.284114676426523.\n",
      "RMSE on training set: 0.28351478360341364.\n",
      "RMSE on training set: 0.2830290096116345.\n",
      "RMSE on training set: 0.2825176352001512.\n",
      "RMSE on training set: 0.281641199205704.\n",
      "RMSE on training set: 0.2809613260266296.\n",
      "RMSE on training set: 0.28017992966024563.\n",
      "RMSE on training set: 0.27953822223126695.\n",
      "RMSE on training set: 0.2786273043627442.\n",
      "RMSE on training set: 0.27806590151007426.\n",
      "RMSE on training set: 0.2773099895447946.\n",
      "RMSE on training set: 0.27691441535648365.\n",
      "RMSE on training set: 0.27629044858396634.\n",
      "RMSE on training set: 0.275791950097862.\n",
      "RMSE on training set: 0.2757581209716878.\n",
      "RMSE on training set: 0.2753518472763324.\n",
      "RMSE on training set: 0.27532643526508577.\n",
      "RMSE on training set: 0.27513153186856315.\n",
      "RMSE on training set: 0.27464791427729296.\n",
      "RMSE on training set: 0.2742570045814559.\n",
      "RMSE on training set: 0.2740868699468559.\n",
      "RMSE on training set: 0.27353320155311767.\n",
      "RMSE on training set: 0.2731748497830091.\n",
      "RMSE on training set: 0.27248241191097267.\n",
      "RMSE on training set: 0.27239680368619773.\n",
      "RMSE on training set: 0.2721998025918088.\n",
      "RMSE on training set: 0.2719940841072578.\n",
      "RMSE on training set: 0.2717710466904073.\n",
      "RMSE on training set: 0.27159076736928134.\n",
      "RMSE on training set: 0.2711524471476858.\n",
      "RMSE on training set: 0.2709631489353814.\n",
      "RMSE on training set: 0.2706789533953026.\n",
      "RMSE on training set: 0.2705582954039051.\n",
      "RMSE on training set: 0.2703599546612433.\n",
      "RMSE on training set: 0.2702391542414235.\n",
      "RMSE on training set: 0.2702477846335771.\n"
     ]
    }
   ],
   "source": [
    "item_features, user_features = ALS(train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE after running ALS: 1.295293333633461.\n"
     ]
    }
   ],
   "source": [
    "rmse = compute_error(test, user_features, item_features)\n",
    "print(\"test RMSE after running ALS: {v}.\".format(v=rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_item_list = [0.01, 0.08,0.1,0.2,0.3]\n",
    "lambda_item_error = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'K': 50, 'stop_criterion': 1e-05, 'lambda_user': 0.15, 'lambda_item': 0.01}\n",
      "\n",
      "start the ALS algorithm...\n",
      "RMSE on training set: 0.8338565388042788.\n",
      "RMSE on training set: 0.6555610107596767.\n",
      "RMSE on training set: 0.5999930030378475.\n",
      "RMSE on training set: 0.5723965752436729.\n",
      "RMSE on training set: 0.5568861758860546.\n",
      "RMSE on training set: 0.5463923868047965.\n",
      "RMSE on training set: 0.5393508483582634.\n",
      "RMSE on training set: 0.5340316936282168.\n",
      "RMSE on training set: 0.5292586792015938.\n",
      "RMSE on training set: 0.5259875806283619.\n",
      "RMSE on training set: 0.5230662323472631.\n",
      "RMSE on training set: 0.5201643478579974.\n",
      "RMSE on training set: 0.5179221044506314.\n",
      "RMSE on training set: 0.5158057795349893.\n",
      "RMSE on training set: 0.5143069267367966.\n",
      "RMSE on training set: 0.5122894944935782.\n",
      "RMSE on training set: 0.5110587836292976.\n",
      "RMSE on training set: 0.5097885027802581.\n",
      "RMSE on training set: 0.5089093353766935.\n",
      "RMSE on training set: 0.5077852707056687.\n",
      "RMSE on training set: 0.5071280324630296.\n",
      "RMSE on training set: 0.5060737541600903.\n",
      "RMSE on training set: 0.5053450658711688.\n",
      "RMSE on training set: 0.5048232702556632.\n",
      "RMSE on training set: 0.5042084296183283.\n",
      "RMSE on training set: 0.5036021009023017.\n",
      "RMSE on training set: 0.5026564380657885.\n",
      "RMSE on training set: 0.5020203607211552.\n",
      "RMSE on training set: 0.5014532477128427.\n",
      "RMSE on training set: 0.5009646446077162.\n",
      "RMSE on training set: 0.5003170932199652.\n",
      "RMSE on training set: 0.49948662888409306.\n",
      "RMSE on training set: 0.49894935794808815.\n",
      "RMSE on training set: 0.4985425160989205.\n",
      "RMSE on training set: 0.4980229594888188.\n",
      "RMSE on training set: 0.49773720647504455.\n",
      "RMSE on training set: 0.4976247340797154.\n",
      "RMSE on training set: 0.49736688891089775.\n",
      "RMSE on training set: 0.4970526058990802.\n",
      "RMSE on training set: 0.49670056066340024.\n",
      "RMSE on training set: 0.4963012741805545.\n",
      "RMSE on training set: 0.4961602728213433.\n",
      "RMSE on training set: 0.49573232336679157.\n",
      "RMSE on training set: 0.49561469013208953.\n",
      "RMSE on training set: 0.49526162278839747.\n",
      "RMSE on training set: 0.49512974643862556.\n",
      "RMSE on training set: 0.49474804906662245.\n",
      "RMSE on training set: 0.49443681852753085.\n",
      "RMSE on training set: 0.49436133919213787.\n",
      "RMSE on training set: 0.49417731001395065.\n",
      "RMSE on training set: 0.49418202956690027.\n",
      "test RMSE after running ALS: 1.1299988852160197.\n",
      "\n",
      "{'K': 50, 'stop_criterion': 1e-05, 'lambda_user': 0.15, 'lambda_item': 0.08}\n",
      "\n",
      "start the ALS algorithm...\n",
      "RMSE on training set: 1.0692179292028092.\n",
      "RMSE on training set: 0.9803579865960385.\n",
      "RMSE on training set: 0.9146550708789509.\n",
      "RMSE on training set: 0.8802997776208601.\n",
      "RMSE on training set: 0.8619032045775287.\n",
      "RMSE on training set: 0.8512076887225722.\n",
      "RMSE on training set: 0.8438131966775642.\n",
      "RMSE on training set: 0.8383337242149321.\n",
      "RMSE on training set: 0.8341753372820379.\n",
      "RMSE on training set: 0.8308358882233059.\n",
      "RMSE on training set: 0.8285476915491213.\n",
      "RMSE on training set: 0.8262108156937914.\n",
      "RMSE on training set: 0.824376720516764.\n",
      "RMSE on training set: 0.8232895999969854.\n",
      "RMSE on training set: 0.8222237350686908.\n",
      "RMSE on training set: 0.821363800271645.\n",
      "RMSE on training set: 0.8208752516357266.\n",
      "RMSE on training set: 0.8200366562223171.\n",
      "RMSE on training set: 0.8198034027624511.\n",
      "RMSE on training set: 0.8193082303405476.\n",
      "RMSE on training set: 0.8189124464342324.\n",
      "RMSE on training set: 0.8186389875630191.\n",
      "RMSE on training set: 0.8183255369302614.\n",
      "RMSE on training set: 0.8180091149932011.\n",
      "RMSE on training set: 0.817846580478567.\n",
      "RMSE on training set: 0.8176012918141424.\n",
      "RMSE on training set: 0.8173930239100998.\n",
      "RMSE on training set: 0.8172703207169166.\n",
      "RMSE on training set: 0.8172018272024715.\n",
      "RMSE on training set: 0.8171133479155323.\n",
      "RMSE on training set: 0.8169220857510774.\n",
      "RMSE on training set: 0.816864983888502.\n",
      "RMSE on training set: 0.8167821790969162.\n",
      "RMSE on training set: 0.8167193560685985.\n",
      "RMSE on training set: 0.816593695512465.\n",
      "RMSE on training set: 0.8164822983969001.\n",
      "RMSE on training set: 0.8163223169791735.\n",
      "RMSE on training set: 0.8162908883735293.\n",
      "RMSE on training set: 0.8161994528214784.\n",
      "RMSE on training set: 0.816125153892425.\n",
      "RMSE on training set: 0.816116580503806.\n",
      "test RMSE after running ALS: 1.0367729190105692.\n",
      "\n",
      "{'K': 50, 'stop_criterion': 1e-05, 'lambda_user': 0.15, 'lambda_item': 0.1}\n",
      "\n",
      "start the ALS algorithm...\n",
      "RMSE on training set: 1.1334625632187816.\n",
      "RMSE on training set: 1.03646096376808.\n",
      "RMSE on training set: 0.9711170800911215.\n",
      "RMSE on training set: 0.9307338555367674.\n",
      "RMSE on training set: 0.9087199753808236.\n",
      "RMSE on training set: 0.895235181621726.\n",
      "RMSE on training set: 0.8861041251291796.\n",
      "RMSE on training set: 0.8801805444559074.\n",
      "RMSE on training set: 0.875989172064161.\n",
      "RMSE on training set: 0.8725772103173995.\n",
      "RMSE on training set: 0.8697446901839535.\n",
      "RMSE on training set: 0.8674435149925234.\n",
      "RMSE on training set: 0.8653060429691223.\n",
      "RMSE on training set: 0.863884422996674.\n",
      "RMSE on training set: 0.8627119181497944.\n",
      "RMSE on training set: 0.8618166083369758.\n",
      "RMSE on training set: 0.8611939416479168.\n",
      "RMSE on training set: 0.8606168963590721.\n",
      "RMSE on training set: 0.8599011478887044.\n",
      "RMSE on training set: 0.8593639453232279.\n",
      "RMSE on training set: 0.8590001933724031.\n",
      "RMSE on training set: 0.8587096240816978.\n",
      "RMSE on training set: 0.858500461965122.\n",
      "RMSE on training set: 0.8580738305992164.\n",
      "RMSE on training set: 0.8578617947249855.\n",
      "RMSE on training set: 0.8575599609756349.\n",
      "RMSE on training set: 0.8574212449796634.\n",
      "RMSE on training set: 0.8571981643558804.\n",
      "RMSE on training set: 0.8570838810625026.\n",
      "RMSE on training set: 0.8568716002218317.\n",
      "RMSE on training set: 0.8567899397399753.\n",
      "RMSE on training set: 0.8568062724589783.\n",
      "RMSE on training set: 0.8568253269043252.\n",
      "RMSE on training set: 0.8567491065802663.\n",
      "RMSE on training set: 0.8564795589018344.\n",
      "RMSE on training set: 0.8564604967638764.\n",
      "RMSE on training set: 0.8564496039231264.\n",
      "RMSE on training set: 0.8564087545363687.\n",
      "RMSE on training set: 0.8563896908223962.\n",
      "RMSE on training set: 0.8564359876773617.\n",
      "RMSE on training set: 0.8564005844252005.\n",
      "RMSE on training set: 0.8562916421607587.\n",
      "RMSE on training set: 0.8562644044289152.\n",
      "RMSE on training set: 0.8562889184265626.\n",
      "RMSE on training set: 0.8562889184265626.\n",
      "test RMSE after running ALS: 1.0361475029725915.\n",
      "\n",
      "{'K': 50, 'stop_criterion': 1e-05, 'lambda_user': 0.15, 'lambda_item': 0.2}\n",
      "\n",
      "start the ALS algorithm...\n",
      "RMSE on training set: 1.4125287355889906.\n",
      "RMSE on training set: 1.186986689427033.\n",
      "RMSE on training set: 1.1088614224774855.\n",
      "RMSE on training set: 1.0737541786635367.\n",
      "RMSE on training set: 1.0530410340289784.\n",
      "RMSE on training set: 1.0398308760417057.\n",
      "RMSE on training set: 1.0307928106332591.\n",
      "RMSE on training set: 1.0242921279010968.\n",
      "RMSE on training set: 1.0196524910662212.\n",
      "RMSE on training set: 1.0158460898634623.\n",
      "RMSE on training set: 1.0129859318770427.\n",
      "RMSE on training set: 1.0109623963734946.\n",
      "RMSE on training set: 1.0093323284381308.\n",
      "RMSE on training set: 1.0078361691363265.\n",
      "RMSE on training set: 1.006893860257957.\n",
      "RMSE on training set: 1.0058022730735026.\n",
      "RMSE on training set: 1.005087812748226.\n",
      "RMSE on training set: 1.0045423470364292.\n",
      "RMSE on training set: 1.0039780006205377.\n",
      "RMSE on training set: 1.0035760297392093.\n",
      "RMSE on training set: 1.0032413184359348.\n",
      "RMSE on training set: 1.002943703502252.\n",
      "RMSE on training set: 1.0026971742683601.\n",
      "RMSE on training set: 1.0025762132354135.\n",
      "RMSE on training set: 1.0024622173667583.\n",
      "RMSE on training set: 1.0023365742499433.\n",
      "RMSE on training set: 1.0021853162732657.\n",
      "RMSE on training set: 1.0021387707643177.\n",
      "RMSE on training set: 1.0020386905987475.\n",
      "RMSE on training set: 1.0019711890289045.\n",
      "RMSE on training set: 1.0019083386516023.\n",
      "RMSE on training set: 1.0018524683394956.\n",
      "RMSE on training set: 1.0018478123395127.\n",
      "test RMSE after running ALS: 1.0463089370656575.\n",
      "\n",
      "{'K': 50, 'stop_criterion': 1e-05, 'lambda_user': 0.15, 'lambda_item': 0.3}\n",
      "\n",
      "start the ALS algorithm...\n",
      "RMSE on training set: 1.6022814106195746.\n",
      "RMSE on training set: 1.2554350541185388.\n",
      "RMSE on training set: 1.1470259104397236.\n",
      "RMSE on training set: 1.1049190744224138.\n",
      "RMSE on training set: 1.0845689308828486.\n",
      "RMSE on training set: 1.0714340356770289.\n",
      "RMSE on training set: 1.0635072507310404.\n",
      "RMSE on training set: 1.0583212401600102.\n",
      "RMSE on training set: 1.0545172910125753.\n",
      "RMSE on training set: 1.0513031731082525.\n",
      "RMSE on training set: 1.0495224348101784.\n",
      "RMSE on training set: 1.0476184569696758.\n",
      "RMSE on training set: 1.0465938604992766.\n",
      "RMSE on training set: 1.0457578488809578.\n",
      "RMSE on training set: 1.0450707044555962.\n",
      "RMSE on training set: 1.0446153339900244.\n",
      "RMSE on training set: 1.0443339765675075.\n",
      "RMSE on training set: 1.0439564812803988.\n",
      "RMSE on training set: 1.0438403014192634.\n",
      "RMSE on training set: 1.043695058406282.\n",
      "RMSE on training set: 1.043569909754231.\n",
      "RMSE on training set: 1.0434201586085132.\n",
      "RMSE on training set: 1.0432569724432026.\n",
      "RMSE on training set: 1.0432234378779128.\n",
      "RMSE on training set: 1.0431228277135791.\n",
      "RMSE on training set: 1.043095996697493.\n",
      "RMSE on training set: 1.0430825809306365.\n",
      "RMSE on training set: 1.043069164991229.\n",
      "RMSE on training set: 1.0430378604615413.\n",
      "RMSE on training set: 1.0430378604615413.\n",
      "test RMSE after running ALS: 1.0602178470917187.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lambda_item in lambda_item_list:\n",
    "    params['lambda_item'] = lambda_item\n",
    "    print(params)\n",
    "    item_features, user_features = ALS(train, params)\n",
    "    rmse = compute_error(test, user_features, item_features)\n",
    "    print(\"test RMSE after running ALS: {v}.\\n\".format(v=rmse))\n",
    "    lambda_item_error.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda_item = lambda_item_list[int(lambda_item_error.index(min(lambda_item_error)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(best_lambda_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['lambda_item'] = best_lambda_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'K': 50, 'stop_criterion': 1e-05, 'lambda_user': 0.15, 'lambda_item': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit over the full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start the ALS algorithm...\n",
      "RMSE on training set: 1.1726405309746673.\n",
      "RMSE on training set: 1.108821128508325.\n",
      "RMSE on training set: 1.0794240445974472.\n",
      "RMSE on training set: 1.0595128339955782.\n",
      "RMSE on training set: 1.0468460551472827.\n",
      "RMSE on training set: 1.0387344394929414.\n",
      "RMSE on training set: 1.0329648236600435.\n",
      "RMSE on training set: 1.0289223671577747.\n",
      "RMSE on training set: 1.0259341110931615.\n",
      "RMSE on training set: 1.023697258164516.\n",
      "RMSE on training set: 1.0218580213956034.\n",
      "RMSE on training set: 1.020391489309146.\n",
      "RMSE on training set: 1.0190875230743592.\n",
      "RMSE on training set: 1.018101567426348.\n",
      "RMSE on training set: 1.0171948469944276.\n",
      "RMSE on training set: 1.0164929615352012.\n",
      "RMSE on training set: 1.0158662864863277.\n",
      "RMSE on training set: 1.0153651700434885.\n",
      "RMSE on training set: 1.014929524878432.\n",
      "RMSE on training set: 1.014588745985107.\n",
      "RMSE on training set: 1.014271727236678.\n",
      "RMSE on training set: 1.0140249954778517.\n",
      "RMSE on training set: 1.013773175028898.\n",
      "RMSE on training set: 1.0135870977755612.\n",
      "RMSE on training set: 1.0134496133579511.\n",
      "RMSE on training set: 1.0133095948128306.\n",
      "RMSE on training set: 1.0132119057475852.\n",
      "RMSE on training set: 1.0130810798865535.\n",
      "RMSE on training set: 1.0130026601346322.\n",
      "RMSE on training set: 1.0128869065196247.\n",
      "RMSE on training set: 1.0127883377602456.\n",
      "RMSE on training set: 1.012718285193539.\n",
      "RMSE on training set: 1.0126352225779691.\n",
      "RMSE on training set: 1.01261214842002.\n",
      "RMSE on training set: 1.012597464591201.\n",
      "RMSE on training set: 1.012554250945269.\n",
      "RMSE on training set: 1.0125164899389691.\n",
      "RMSE on training set: 1.012479986294195.\n",
      "RMSE on training set: 1.0124506145900325.\n",
      "RMSE on training set: 1.0124392852764224.\n",
      "RMSE on training set: 1.0124090731536486.\n",
      "RMSE on training set: 1.0123939667541624.\n",
      "RMSE on training set: 1.012372146001395.\n",
      "RMSE on training set: 1.0123515837077939.\n",
      "RMSE on training set: 1.0123452890446247.\n"
     ]
    }
   ],
   "source": [
    "item_features_full, user_features_full = ALS(ratings_full, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record the result into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000)\n",
      "Submission created successfully!\n"
     ]
    }
   ],
   "source": [
    "from helpers import get_submission_csv\n",
    "\n",
    "get_submission_csv(item_features_full, user_features_full, path_sample, path_submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
